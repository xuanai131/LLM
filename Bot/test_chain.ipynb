{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper_Utilities import book_researcher_checktool_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email sent successfully!\n"
     ]
    }
   ],
   "source": [
    "# ojeq xnrh rwbg sqxj\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# Email account credentials\n",
    "sender_email = 'ronin792002@gmail.com'\n",
    "receiver_email = 'vuvu3921@gmail.com'\n",
    "password = 'Lamvu2002'\n",
    "\n",
    "# Create the MIMEMultipart object\n",
    "msg = MIMEMultipart()\n",
    "msg['From'] = sender_email\n",
    "msg['To'] = receiver_email\n",
    "msg['Subject'] = 'Test Email from Python'\n",
    "\n",
    "# Email body\n",
    "body = 'This is a test email sent from Python script.'\n",
    "\n",
    "# Attach the body with the msg instance\n",
    "msg.attach(MIMEText(body, 'plain'))\n",
    "\n",
    "try:\n",
    "    # Create SMTP session\n",
    "    server = smtplib.SMTP('smtp.gmail.com', 587)  # Use Gmail's SMTP server\n",
    "    server.starttls()  # Enable security\n",
    "\n",
    "    # Login to the server\n",
    "    server.login(sender_email, \"ojeq xnrh rwbg sqxj\")\n",
    "\n",
    "    # Send the email\n",
    "    server.send_message(msg)\n",
    "\n",
    "    # Terminate the session\n",
    "    server.quit()\n",
    "\n",
    "    print('Email sent successfully!')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Failed to send email: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iVBORw0KGgoAAAANSUhEUgAAAgsAAADICAIAAADp3jWqAAADGElEQVR4nO3cwYqDMBRA0ab0/385sxsKc4taw9jCOSsRmjzUcnHjmHPeAOCP+9UDAPChFAKAphAANIUAoCkEAE0hAGgKAUBTCACaQgDQFAKA9rh6gP8wxvg9fv7KyJ7zm84seHTT/ETKmcX3DL9pzw/fHnLPgks2XfVgHBp41b3b3OiVo5dx+aavHN1o8+FZdRnf/pt8Ke8QADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQADSFAKApBABNIQBoCgFAUwgAmkIA0BQCgKYQALQx57x6BgA+kXcIAJpCANAUAoCmEAA0hQCgKQQATSEAaAoBQFMIAJpCANB+AOiCYYssXBoXAAAAAElFTkSuQmCC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lamvu/Python_env/LLM/lib/python3.10/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import barcode\n",
    "from barcode.writer import ImageWriter\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import uuid\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from table_handle import *\n",
    "from Database_handle import *\n",
    "\n",
    "\n",
    "def generate_random_unique_id():\n",
    "    # Generate a random unique ID\n",
    "    unique_id = uuid.uuid4().int\n",
    "    \n",
    "    # Convert the ID to a string and ensure it fits the length for EAN13\n",
    "    unique_id_str = str(unique_id)[:12]  # EAN13 requires a 12-digit number + 1 check digit\n",
    "    return unique_id_str\n",
    "def generate_barcode_base64():\n",
    "    existing_IDs = SearchAllAccountBarcode()\n",
    "    # existing_IDs = []\n",
    "    while True:\n",
    "        unique_id = uuid.uuid4().int\n",
    "    \n",
    "    # Convert the ID to a string and ensure it fits the length for EAN13\n",
    "        id = str(unique_id)[:12]\n",
    "        if id not in existing_IDs:\n",
    "            break\n",
    "    # Generate barcode\n",
    "    EAN = barcode.get_barcode_class('ean13')  # You can change the barcode type if needed\n",
    "    ean = EAN(id, writer=ImageWriter())\n",
    "    \n",
    "    # Save barcode to a BytesIO object\n",
    "    buffer = BytesIO()\n",
    "    ean.write(buffer, options={'write_text': False})  # You can add options as needed\n",
    "    \n",
    "    # Rewind the buffer to the beginning\n",
    "    buffer.seek(0)\n",
    "    \n",
    "    # Open the image with PIL and convert it to Base64\n",
    "    image = Image.open(buffer)\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    \n",
    "    return img_str\n",
    "\n",
    "# Generate barcode and get the Base64 encoded string\n",
    "barcode_base64 = generate_barcode_base64()\n",
    "print(barcode_base64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ' tôi đã có thông tin sách mà bạn muốn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_researcher_checktool_chain.invoke({'messages': [res]})['messages'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite error: 6 values for 3 columns\n"
     ]
    }
   ],
   "source": [
    "from Database_handle import *\n",
    "InsertUserInfo( \"ronin792002@gmail.com\", \"12345678\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "ID:  {'id': '7,8,9'}\n",
      "=============================\n",
      "Request succeeded with status 200 (OK)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Load the book sucessfully '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_book(\"7,8,9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================\n",
      "ID:  {'id': '5,6,7'}\n",
      "=============================\n",
      "Request succeeded with status 200 (OK)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Load the book sucessfully '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_book(\"5,6,7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from API_keys import *\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "### Construct retriever ###\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context to answer the question. \\\n",
    "If you don't know the answer, just say that you don't know. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,AIMessage\n",
    "\n",
    "OpenAIHistoryConversation = [HumanMessage(content='thư viện có sách về cơ khí không'),\n",
    "AIMessage(content='Đã tìm thấy sách cơ khí bạn cần , đó là cuốn Cẩm nang cơ khí có sẵn ở trong thư viện')]\n",
    "\n",
    "second_question = ' thư viện có cuốn đó không?'\n",
    "# ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": OpenAIHistoryConversation})\n",
    "\n",
    "# print(ai_msg_2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n",
    "conversation_with_kg = ConversationChain(\n",
    "    llm=llm, verbose=True, prompt=prompt, memory=ConversationKGMemory(llm=llm)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human:  thư viện có cuốn đó không?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ai_msg_2 = conversation_with_kg.invoke({\"input\": second_question, \"history\": OpenAIHistoryConversation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human: thư viện có sách về cơ khí không\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Có, thư viện chứa một bộ sưu tập sách về cơ khí. Có sách về cơ khí cơ bản, cơ khí ứng dụng và cơ khí chuyên sâu. Bạn có thể tìm thấy thông tin chi tiết về cơ khí trong các cuốn sách này. Nếu bạn cần thêm thông tin cụ thể, hãy cho tôi biết!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(input=\"thư viện có sách về cơ khí không\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "On thư viện: thư viện có sách về cơ khí.\n",
      "\n",
      "Conversation:\n",
      "Human: Đã tìm thấy sách cơ khí bạn cần , đó là cuốn Cẩm nang cơ khí có sẵn ở trong thư viện\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Đúng vậy, thư viện chúng tôi có rất nhiều sách về cơ khí, trong đó có cuốn Cẩm nang cơ khí mà bạn cần. Bạn có thể tìm thấy nó ở kệ sách số 3, hàng số 5. Chúc bạn tìm được thông tin hữu ích từ cuốn sách đó!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(input='Đã tìm thấy sách cơ khí bạn cần , đó là cuốn Cẩm nang cơ khí có sẵn ở trong thư viện')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. \n",
      "If the AI does not know the answer to a question, it truthfully says it does not know. The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
      "\n",
      "Relevant Information:\n",
      "\n",
      "\n",
      "\n",
      "Conversation:\n",
      "Human:  tôi tìm cuốn sách Cẩm nang cơ khí ở kệ nào trong thư viện?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Xin lỗi, tôi không biết cuốn sách Cẩm nang cơ khí ở kệ nào trong thư viện.Bạn có thể hỏi thủ thư hoặc tìm trên hệ thống máy tính của thư viện để biết chính xác vị trí của cuốn sách đó.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_kg.predict(input=' tôi tìm cuốn sách Cẩm nang cơ khí ở kệ nào trong thư viện?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
